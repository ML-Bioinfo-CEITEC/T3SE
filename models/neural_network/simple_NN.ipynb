{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50847be6-184f-41e7-adc2-789e135fdca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2cb8a-5bab-4601-9243-b165c8c34475",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65d43c42-c874-4a6b-9414-58cf838bd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.lin1 = nn.Linear(1024, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.loss = nn.functional.binary_cross_entropy\n",
    " \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lin2(X)\n",
    "        X = self.sigmoid(X)\n",
    "        return X\n",
    "    \n",
    "    def train_model(self, dataset, epochs):  \n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            with tqdm(dataset, unit=\"batch\") as tepoch:\n",
    "                for inputs, targets in tepoch:\n",
    "                    \n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                    tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                    \n",
    "                    # clear the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # compute the model output\n",
    "                    yhat = self(inputs.float())\n",
    "                    # calculate accuracy\n",
    "                    correct = (torch.round(yhat) == targets).sum().item()\n",
    "                    accuracy = correct / len(inputs)\n",
    "                    # calculate loss\n",
    "                    loss = self.loss(yhat, targets.float())\n",
    "                    # credit assignment\n",
    "                    loss.backward()\n",
    "                    # update model weights\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "                \n",
    "    def test(self, dataloader):\n",
    "        model.eval()\n",
    "        predictions, actuals = list(), list()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                # evaluate the model on the test set\n",
    "                yhat = self(inputs.float())\n",
    "                yhat = yhat.cpu().detach().numpy()\n",
    "                actual = targets.numpy()\n",
    "                # reshape for stacking\n",
    "                actual = actual.reshape((len(actual), 1))\n",
    "                yhat = yhat.reshape((len(yhat), 1))\n",
    "                # store\n",
    "                predictions.append(yhat)\n",
    "                actuals.append(actual)\n",
    "        predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "        print(\"Predictions: \", predictions[:10])\n",
    "        print(\"Real labels: \", actuals[:10])\n",
    "        # calculate accuracy\n",
    "        pred_label = np.round(predictions)\n",
    "        acc = metrics.accuracy_score(actuals, pred_label)\n",
    "        f1 = metrics.f1_score(actuals, pred_label, average='binary', zero_division=0)\n",
    "        auroc = metrics.roc_auc_score(actuals, predictions)\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(actuals, predictions)\n",
    "        specificity = metrics.recall_score(actuals, pred_label, pos_label=0)\n",
    "        sensitivity = metrics.recall_score(actuals, pred_label)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        print(f\"Test metrics: \\n Accuracy: {float(acc):>6f}, F1 score: {float(f1):>6f}, AUROC: {float(auroc):>6f}, AUPRC: {float(auprc):>6f}, Specificity: {float(specificity):>6f}, Sensitivity: {float(sensitivity):>6f}\\n\")\n",
    "        return acc, f1, auroc, auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d851d-7b75-4628-89ed-69abe16332cc",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f9fb689-08db-4f17-982b-45674343066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = df.drop(columns=['Label']).to_numpy()\n",
    "        self.y = np.expand_dims(df['Label'].to_numpy(), axis=1)\n",
    "        self.len = len(df)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c54c6f5-e6f6-4697-bb32-617f0b893119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('T3SE_clustered_embeddings_Nterm_train.csv')\n",
    "train_df.drop(columns=['ID', 'Sequence_part'], inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('T3SE_clustered_embeddings_Nterm_test.csv')\n",
    "test_df.drop(columns=['ID', 'Sequence_part'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "972d8146-e40e-48c1-a843-565d7891588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3013\n",
      "Test dataset size: 754\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "852e5582-78f2-4c6d-88af-91ae653d1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = EmbeddingDataset(train_df)\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True)\n",
    "test_dset = EmbeddingDataset(test_df)\n",
    "test_loader = DataLoader(test_dset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42df7a93-0ae0-46f2-a54d-a09c8a35e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283db3f8-11ce-49e8-922c-cea3f4ecb403",
   "metadata": {},
   "source": [
    "## Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2076ab2-ee1c-4547-8c28-a9b35652b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (lin1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NN(device=device).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2fd1674b-d912-4f67-9fab-45bbbc03b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 95/95 [00:00<00:00, 260.25batch/s, accuracy=100, loss=0.142] \n",
      "Epoch 2: 100%|██████████| 95/95 [00:00<00:00, 292.64batch/s, accuracy=60, loss=0.959]  \n",
      "Epoch 3: 100%|██████████| 95/95 [00:00<00:00, 310.90batch/s, accuracy=100, loss=0.0603]\n",
      "Epoch 4: 100%|██████████| 95/95 [00:00<00:00, 244.86batch/s, accuracy=100, loss=0.0719]\n",
      "Epoch 5: 100%|██████████| 95/95 [00:00<00:00, 397.50batch/s, accuracy=80, loss=0.397]  \n",
      "Epoch 6: 100%|██████████| 95/95 [00:00<00:00, 234.23batch/s, accuracy=80, loss=0.357]  \n",
      "Epoch 7: 100%|██████████| 95/95 [00:00<00:00, 236.66batch/s, accuracy=100, loss=0.0658] \n",
      "Epoch 8: 100%|██████████| 95/95 [00:00<00:00, 231.65batch/s, accuracy=100, loss=0.0696] \n",
      "Epoch 9: 100%|██████████| 95/95 [00:00<00:00, 236.32batch/s, accuracy=100, loss=0.0861] \n",
      "Epoch 10: 100%|██████████| 95/95 [00:00<00:00, 236.88batch/s, accuracy=100, loss=0.0574] \n",
      "Epoch 11: 100%|██████████| 95/95 [00:00<00:00, 254.60batch/s, accuracy=80, loss=0.321]   \n",
      "Epoch 12: 100%|██████████| 95/95 [00:00<00:00, 237.18batch/s, accuracy=80, loss=0.54]    \n",
      "Epoch 13: 100%|██████████| 95/95 [00:00<00:00, 320.11batch/s, accuracy=100, loss=0.0316] \n",
      "Epoch 14: 100%|██████████| 95/95 [00:00<00:00, 287.92batch/s, accuracy=100, loss=0.0355] \n",
      "Epoch 15: 100%|██████████| 95/95 [00:00<00:00, 268.81batch/s, accuracy=100, loss=0.0634] \n",
      "Epoch 16: 100%|██████████| 95/95 [00:00<00:00, 249.30batch/s, accuracy=100, loss=0.0608] \n",
      "Epoch 17: 100%|██████████| 95/95 [00:00<00:00, 247.24batch/s, accuracy=100, loss=0.0205] \n",
      "Epoch 18: 100%|██████████| 95/95 [00:00<00:00, 244.07batch/s, accuracy=100, loss=0.011]  \n",
      "Epoch 19: 100%|██████████| 95/95 [00:00<00:00, 243.43batch/s, accuracy=80, loss=0.183]   \n",
      "Epoch 20: 100%|██████████| 95/95 [00:00<00:00, 248.35batch/s, accuracy=100, loss=0.0151] \n",
      "Epoch 21: 100%|██████████| 95/95 [00:00<00:00, 290.65batch/s, accuracy=100, loss=0.0273] \n",
      "Epoch 22: 100%|██████████| 95/95 [00:00<00:00, 394.52batch/s, accuracy=100, loss=0.0167] \n",
      "Epoch 23: 100%|██████████| 95/95 [00:00<00:00, 396.76batch/s, accuracy=100, loss=0.00371]\n",
      "Epoch 24: 100%|██████████| 95/95 [00:00<00:00, 241.49batch/s, accuracy=100, loss=0.0349] \n",
      "Epoch 25: 100%|██████████| 95/95 [00:00<00:00, 211.03batch/s, accuracy=100, loss=0.0544] \n",
      "Epoch 26: 100%|██████████| 95/95 [00:00<00:00, 204.96batch/s, accuracy=100, loss=0.0216] \n",
      "Epoch 27: 100%|██████████| 95/95 [00:00<00:00, 200.88batch/s, accuracy=100, loss=0.0168] \n",
      "Epoch 28: 100%|██████████| 95/95 [00:00<00:00, 202.27batch/s, accuracy=80, loss=0.375]   \n",
      "Epoch 29: 100%|██████████| 95/95 [00:00<00:00, 200.73batch/s, accuracy=100, loss=0.0464] \n",
      "Epoch 30: 100%|██████████| 95/95 [00:00<00:00, 205.35batch/s, accuracy=100, loss=0.016]  \n",
      "Epoch 31: 100%|██████████| 95/95 [00:00<00:00, 200.78batch/s, accuracy=100, loss=0.0265] \n",
      "Epoch 32: 100%|██████████| 95/95 [00:00<00:00, 204.93batch/s, accuracy=100, loss=0.0257] \n",
      "Epoch 33: 100%|██████████| 95/95 [00:00<00:00, 199.88batch/s, accuracy=80, loss=0.167]   \n",
      "Epoch 34: 100%|██████████| 95/95 [00:00<00:00, 203.13batch/s, accuracy=100, loss=0.0527] \n",
      "Epoch 35: 100%|██████████| 95/95 [00:00<00:00, 202.10batch/s, accuracy=100, loss=0.0058] \n"
     ]
    }
   ],
   "source": [
    "model.train_model(train_loader, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "effb044d-657c-4598-8375-5ee9d36563af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[0.02375989]\n",
      " [0.00854938]\n",
      " [0.5944409 ]\n",
      " [0.01201478]\n",
      " [0.00165575]\n",
      " [0.01242965]\n",
      " [0.00123633]\n",
      " [0.19943842]\n",
      " [0.10316078]\n",
      " [0.01323742]]\n",
      "Real labels:  [[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Test metrics: \n",
      " Accuracy: 0.937666, F1 score: 0.605042, AUROC: 0.920825, AUPRC: 0.700663, Specificity: 0.988218, Sensitivity: 0.480000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9376657824933687,\n",
       " 0.6050420168067226,\n",
       " 0.9208247422680412,\n",
       " 0.7006628756817251)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a2f755a-c1a5-4093-a1d7-e7be5a2bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"T3SEembedding_simple_NN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ec1ac-8527-44a4-a3a5-4dfb7daad9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
